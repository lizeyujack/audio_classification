{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data succesfully loaded!\n",
      "WARNING:tensorflow:From D:\\anaconda_files\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 143)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               73728     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1004)              65260     \n",
      "=================================================================\n",
      "Total params: 286,764\n",
      "Trainable params: 286,764\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 69822 samples, validate on 29924 samples\n",
      "Epoch 1/50\n",
      "69822/69822 [==============================] - 7s 102us/sample - loss: 7.1051 - acc: 7.7340e-04 - val_loss: 6.9116 - val_acc: 7.6861e-04\n",
      "Epoch 2/50\n",
      "69822/69822 [==============================] - 8s 120us/sample - loss: 6.8932 - acc: 0.0019 - val_loss: 6.8193 - val_acc: 0.0019\n",
      "Epoch 3/50\n",
      "69822/69822 [==============================] - 12s 168us/sample - loss: 6.7032 - acc: 0.0042 - val_loss: 6.5213 - val_acc: 0.0052\n",
      "Epoch 4/50\n",
      "69822/69822 [==============================] - 14s 195us/sample - loss: 6.3955 - acc: 0.0082 - val_loss: 6.2143 - val_acc: 0.0097\n",
      "Epoch 5/50\n",
      "69822/69822 [==============================] - 15s 210us/sample - loss: 5.9543 - acc: 0.0191 - val_loss: 5.7372 - val_acc: 0.0273\n",
      "Epoch 6/50\n",
      "69822/69822 [==============================] - 16s 234us/sample - loss: 5.3969 - acc: 0.0451 - val_loss: 5.1780 - val_acc: 0.0598\n",
      "Epoch 7/50\n",
      "69822/69822 [==============================] - 19s 276us/sample - loss: 4.8726 - acc: 0.0825 - val_loss: 4.7792 - val_acc: 0.0925\n",
      "Epoch 8/50\n",
      "69822/69822 [==============================] - 22s 313us/sample - loss: 4.4535 - acc: 0.1242 - val_loss: 4.4793 - val_acc: 0.1271\n",
      "Epoch 9/50\n",
      "69822/69822 [==============================] - 21s 304us/sample - loss: 4.1116 - acc: 0.1623 - val_loss: 4.2349 - val_acc: 0.1575\n",
      "Epoch 10/50\n",
      "69822/69822 [==============================] - 19s 279us/sample - loss: 3.8371 - acc: 0.2001 - val_loss: 4.0039 - val_acc: 0.1869\n",
      "Epoch 11/50\n",
      "69822/69822 [==============================] - 19s 274us/sample - loss: 3.6069 - acc: 0.2353 - val_loss: 3.8442 - val_acc: 0.2081\n",
      "Epoch 12/50\n",
      "69822/69822 [==============================] - 27s 381us/sample - loss: 3.4165 - acc: 0.2662 - val_loss: 3.7210 - val_acc: 0.2322\n",
      "Epoch 13/50\n",
      "69822/69822 [==============================] - 28s 403us/sample - loss: 3.2555 - acc: 0.2935 - val_loss: 3.5942 - val_acc: 0.2484\n",
      "Epoch 14/50\n",
      "69822/69822 [==============================] - 25s 356us/sample - loss: 3.1125 - acc: 0.3173 - val_loss: 3.4943 - val_acc: 0.2696\n",
      "Epoch 15/50\n",
      "69822/69822 [==============================] - 26s 367us/sample - loss: 2.9869 - acc: 0.3397 - val_loss: 3.4693 - val_acc: 0.2748\n",
      "Epoch 16/50\n",
      "69822/69822 [==============================] - 23s 324us/sample - loss: 2.8740 - acc: 0.3580 - val_loss: 3.4003 - val_acc: 0.2874\n",
      "Epoch 17/50\n",
      "69822/69822 [==============================] - 25s 352us/sample - loss: 2.7727 - acc: 0.3769 - val_loss: 3.3432 - val_acc: 0.3033\n",
      "Epoch 18/50\n",
      "69822/69822 [==============================] - 24s 351us/sample - loss: 2.6789 - acc: 0.3953 - val_loss: 3.2895 - val_acc: 0.3139\n",
      "Epoch 19/50\n",
      "69822/69822 [==============================] - 23s 326us/sample - loss: 2.5972 - acc: 0.4115 - val_loss: 3.2151 - val_acc: 0.3248\n",
      "Epoch 20/50\n",
      "69822/69822 [==============================] - 24s 346us/sample - loss: 2.5130 - acc: 0.4265 - val_loss: 3.2577 - val_acc: 0.3253\n",
      "Epoch 21/50\n",
      "69822/69822 [==============================] - 23s 333us/sample - loss: 2.4425 - acc: 0.4413 - val_loss: 3.1736 - val_acc: 0.3389\n",
      "Epoch 22/50\n",
      "69822/69822 [==============================] - 30s 428us/sample - loss: 2.3759 - acc: 0.4533 - val_loss: 3.1997 - val_acc: 0.3380\n",
      "Epoch 23/50\n",
      "69822/69822 [==============================] - 29s 412us/sample - loss: 2.3088 - acc: 0.4698 - val_loss: 3.1609 - val_acc: 0.3465\n",
      "Epoch 24/50\n",
      "69822/69822 [==============================] - 19s 270us/sample - loss: 2.2485 - acc: 0.4806 - val_loss: 3.1596 - val_acc: 0.3481\n",
      "Epoch 25/50\n",
      "69822/69822 [==============================] - 21s 305us/sample - loss: 2.1896 - acc: 0.4909 - val_loss: 3.1054 - val_acc: 0.3577\n",
      "Epoch 26/50\n",
      "69822/69822 [==============================] - 22s 310us/sample - loss: 2.1338 - acc: 0.5025 - val_loss: 3.1112 - val_acc: 0.3642\n",
      "Epoch 27/50\n",
      "69822/69822 [==============================] - 25s 355us/sample - loss: 2.0829 - acc: 0.5123 - val_loss: 3.0747 - val_acc: 0.3697\n",
      "Epoch 28/50\n",
      "69822/69822 [==============================] - 25s 355us/sample - loss: 2.0296 - acc: 0.5236 - val_loss: 3.1300 - val_acc: 0.3696\n",
      "Epoch 29/50\n",
      "69822/69822 [==============================] - 24s 350us/sample - loss: 1.9819 - acc: 0.5324 - val_loss: 3.1241 - val_acc: 0.3647\n",
      "Epoch 30/50\n",
      "69822/69822 [==============================] - 21s 304us/sample - loss: 1.9366 - acc: 0.5414 - val_loss: 3.1439 - val_acc: 0.3722\n",
      "Epoch 31/50\n",
      "69822/69822 [==============================] - 28s 394us/sample - loss: 1.8926 - acc: 0.5497 - val_loss: 3.0788 - val_acc: 0.3822 1.8883 - acc: 0.55 - ETA: 2s - loss: 1.88\n",
      "Epoch 32/50\n",
      "69822/69822 [==============================] - 22s 309us/sample - loss: 1.8495 - acc: 0.5575 - val_loss: 3.1163 - val_acc: 0.3820\n",
      "Epoch 33/50\n",
      "69822/69822 [==============================] - 18s 257us/sample - loss: 1.8081 - acc: 0.5678 - val_loss: 3.0885 - val_acc: 0.3873\n",
      "Epoch 34/50\n",
      "69822/69822 [==============================] - 18s 255us/sample - loss: 1.7702 - acc: 0.5746 - val_loss: 3.1310 - val_acc: 0.3827\n",
      "Epoch 35/50\n",
      "69822/69822 [==============================] - 18s 257us/sample - loss: 1.7293 - acc: 0.5849 - val_loss: 3.1285 - val_acc: 0.3886\n",
      "Epoch 36/50\n",
      "69822/69822 [==============================] - 18s 264us/sample - loss: 1.6925 - acc: 0.5907 - val_loss: 3.1386 - val_acc: 0.3922\n",
      "Epoch 37/50\n",
      "69822/69822 [==============================] - 18s 256us/sample - loss: 1.6585 - acc: 0.5997 - val_loss: 3.1503 - val_acc: 0.3928\n",
      "Epoch 38/50\n",
      "69822/69822 [==============================] - 18s 253us/sample - loss: 1.6241 - acc: 0.6075 - val_loss: 3.1664 - val_acc: 0.3957\n",
      "Epoch 39/50\n",
      "69822/69822 [==============================] - 24s 340us/sample - loss: 1.5896 - acc: 0.6136 - val_loss: 3.2274 - val_acc: 0.3883\n",
      "Epoch 40/50\n",
      "69822/69822 [==============================] - 22s 317us/sample - loss: 1.5565 - acc: 0.6207 - val_loss: 3.1819 - val_acc: 0.3962\n",
      "Epoch 41/50\n",
      "69822/69822 [==============================] - 23s 324us/sample - loss: 1.5231 - acc: 0.6272 - val_loss: 3.2368 - val_acc: 0.3953\n",
      "Epoch 42/50\n",
      "69822/69822 [==============================] - 21s 304us/sample - loss: 1.4932 - acc: 0.6347 - val_loss: 3.2179 - val_acc: 0.3989\n",
      "Epoch 43/50\n",
      "69822/69822 [==============================] - 21s 294us/sample - loss: 1.4616 - acc: 0.6404 - val_loss: 3.2446 - val_acc: 0.4025\n",
      "Epoch 44/50\n",
      "69822/69822 [==============================] - 21s 299us/sample - loss: 1.4327 - acc: 0.6478 - val_loss: 3.2472 - val_acc: 0.4040\n",
      "Epoch 45/50\n",
      "69822/69822 [==============================] - 21s 298us/sample - loss: 1.4039 - acc: 0.6530 - val_loss: 3.3080 - val_acc: 0.4039\n",
      "Epoch 46/50\n",
      "69822/69822 [==============================] - 21s 308us/sample - loss: 1.3743 - acc: 0.6592 - val_loss: 3.3558 - val_acc: 0.4022\n",
      "Epoch 47/50\n",
      "69822/69822 [==============================] - 22s 321us/sample - loss: 1.3535 - acc: 0.6646 - val_loss: 3.3099 - val_acc: 0.4053\n",
      "Epoch 48/50\n",
      "69822/69822 [==============================] - 21s 298us/sample - loss: 1.3236 - acc: 0.6699 - val_loss: 3.3752 - val_acc: 0.4072\n",
      "Epoch 49/50\n",
      "69822/69822 [==============================] - 20s 287us/sample - loss: 1.2957 - acc: 0.6780 - val_loss: 3.3864 - val_acc: 0.4050\n",
      "Epoch 50/50\n",
      "69822/69822 [==============================] - 22s 317us/sample - loss: 1.2713 - acc: 0.6825 - val_loss: 3.4410 - val_acc: 0.4020\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# path to json file that stores MFCCs and genre labels for each processed segment\n",
    "DATA_PATH = \"data_train.json\"\n",
    "\n",
    "def load_data(data_path):\n",
    "    \"\"\"Loads training dataset from json file.\n",
    "\n",
    "        :param data_path (str): Path to json file containing data\n",
    "        :return X (ndarray): Inputs\n",
    "        :return y (ndarray): Targets\n",
    "    \"\"\"\n",
    "\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # convert lists to numpy arrays\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "\n",
    "    print(\"Data succesfully loaded!\")\n",
    "\n",
    "    return  X, y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # load data\n",
    "    X, y = load_data(DATA_PATH)\n",
    "\n",
    "    # create train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    # build network topology\n",
    "    model = keras.Sequential([\n",
    "\n",
    "        # input layer\n",
    "        keras.layers.Flatten(input_shape=(X.shape[1], X.shape[2])),\n",
    "\n",
    "        # 1st dense layer\n",
    "        keras.layers.Dense(512, activation='relu'),\n",
    "\n",
    "        # 2nd dense layer\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "\n",
    "        # 3rd dense layer\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "\n",
    "        # output layer\n",
    "        keras.layers.Dense(1004, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimiser,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # train model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=50)\n",
    "#     model.save(\"data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data succesfully loaded!\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 143)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               73728     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1004)              65260     \n",
      "=================================================================\n",
      "Total params: 286,764\n",
      "Trainable params: 286,764\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 69822 samples, validate on 29924 samples\n",
      "Epoch 1/2\n",
      "69822/69822 [==============================] - 35s 505us/sample - loss: 8.3826 - acc: 8.7365e-04 - val_loss: 7.3975 - val_acc: 6.6836e-04\n",
      "Epoch 2/2\n",
      "69822/69822 [==============================] - 33s 475us/sample - loss: 7.3614 - acc: 8.5933e-04 - val_loss: 7.3128 - val_acc: 5.0127e-04\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7aa12a0e3116>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m# plot accuracy and error as a function of the epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mplot_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-7aa12a0e3116>\u001b[0m in \u001b[0;36mplot_history\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# create accuracy sublpot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"test accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# path to json file that stores MFCCs and genre labels for each processed segment\n",
    "DATA_PATH = \"data_train.json\"\n",
    "\n",
    "def load_data(data_path):\n",
    "    \"\"\"Loads training dataset from json file.\n",
    "\n",
    "        :param data_path (str): Path to json file containing data\n",
    "        :return X (ndarray): Inputs\n",
    "        :return y (ndarray): Targets\n",
    "    \"\"\"\n",
    "\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # convert lists to numpy arrays\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "\n",
    "    print(\"Data succesfully loaded!\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
    "\n",
    "        :param history: Training history of model\n",
    "        :return:\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    # create accuracy sublpot\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy eval\")\n",
    "\n",
    "    # create error sublpot\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
    "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
    "    axs[1].set_ylabel(\"Error\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Error eval\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # load data\n",
    "    X, y = load_data(DATA_PATH)\n",
    "\n",
    "    # create train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    # build network topology\n",
    "    model = keras.Sequential([\n",
    "\n",
    "        # input layer\n",
    "        keras.layers.Flatten(input_shape=(X.shape[1], X.shape[2])),\n",
    "\n",
    "        # 1st dense layer\n",
    "        keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "\n",
    "        # 2nd dense layer\n",
    "        keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "\n",
    "        # 3rd dense layer\n",
    "        keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "\n",
    "        # output layer\n",
    "        keras.layers.Dense(1004, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimiser,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # train model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=2)\n",
    "\n",
    "    # plot accuracy and error as a function of the epochs\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data_train.json\"\n",
    "with open(data_path, \"r\") as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "X = np.array(data[\"mfcc\"])\n",
    "y = np.array(data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99746, 11, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99746,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-fd85d96e892c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "history.history[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
